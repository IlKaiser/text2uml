{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1354cc70-f0e5-4ceb-8abb-e8290f9e4a05",
   "metadata": {},
   "source": [
    "# Template Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48051682-039d-4dea-80fa-416f0e56d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8abf5dda-e975-442e-a7c1-58657f77f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_class = \"\"\"\n",
    "Step 1:\n",
    "\n",
    "You will be asked to extact, from text specification, the list of class to use in \n",
    "a uml conceptual model diagram. Include only classes relevant for conceptual modeling.\n",
    "\n",
    "Choose the key words from text and output the results in a list form:\n",
    "\n",
    "[ClassName1, ClassName2, ...]\n",
    "\n",
    "i.e. for a Book text specification:\n",
    "\n",
    "[Book, Page, Author, ...]\n",
    "\n",
    "Output only the list and no other text.\n",
    "Keep the list as short as possible and do not add irrelevant classes.\n",
    "\n",
    "##############\n",
    "\n",
    "The specification text is: \n",
    "\n",
    "{text}.\n",
    "\n",
    "##############\n",
    "\n",
    "The class list is:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_assoc =  \"\"\"\n",
    "Step 2:\n",
    "\n",
    "You will be asked to extact, from text specification and a list of classes, the associations amongst\n",
    "the classes to use in a uml conceptual model diagram. \n",
    "\n",
    "Include only associations relevant for conceptual modeling. Use only classes from the list.\n",
    "\n",
    "\n",
    "Choose the key words from class list according to text specifications and output the results in a list form:\n",
    "\n",
    "[(ClassName1, ClassName2), (ClassName1,Classname3), ...]\n",
    "\n",
    "i.e. for a Book text specification:\n",
    "\n",
    "[(Book, Page), (Book, Author), ...]\n",
    "\n",
    "All associations are meant to be symmetrical. If (ClassName1, ClassName2) is in the association list,\n",
    "there is no need to add (Classname2, ClassName1).\n",
    "\n",
    "Output only the list and no other text.\n",
    "Keep the list as short as possible and do not add irrelevant association.\n",
    "\n",
    "##############\n",
    "\n",
    "The class list is: \n",
    "\n",
    "{classes}\n",
    "\n",
    "#############\n",
    "\n",
    "The specification text is: \n",
    "\n",
    "{text}\n",
    "\n",
    "##############\n",
    "\n",
    "The association list is:\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template_card = \"\"\"\n",
    "Step 3:\n",
    "\n",
    "You will be asked to extact, from text specification and a list of association, the cardinality for \n",
    "each association to use in a uml conceptual model diagram. \n",
    "\n",
    "Use only association from the list. The format of the cardinality is:\n",
    "\n",
    "min_cardinality..max_cardinlaity\n",
    "\n",
    "For example 1..100 has 1 as minimum cardinality and 100 as maximum cadinality.\n",
    "Use the asterisk (*) for an undefined number of maximum cardinality,\n",
    "and 0 for an undefined number of minumum cardinality.\n",
    "\n",
    "If you are not sure assign default cardinality: 0..*\n",
    "\n",
    "Decide the correct cardinality for each association from text and output the results in a list form:\n",
    "\n",
    "[(ClassName1, Cadinality1 ,ClassName2, Cardinality2), ...]\n",
    "\n",
    "i.e. for a Book text specification:\n",
    "\n",
    "[(Book, 1..* ,Page, 1..1),  ...]\n",
    "\n",
    "Output only the list and no other text.\n",
    "\n",
    "##############\n",
    "\n",
    "The association list is:\n",
    "\n",
    "{association}\n",
    "\n",
    "#############\n",
    "\n",
    "The specification text is: \n",
    "\n",
    "{text}\n",
    "\n",
    "##############\n",
    "\n",
    "The cardinality list is:\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "prompt_template_plant = \"\"\"\n",
    "Step 4:\n",
    "\n",
    "You will be asked by the user to create a plant UMl model from a class list and a cardinality list.\n",
    "Do so in the most clear way possible, avoid class properties and assign molteplicity. \n",
    "\n",
    "The format of class list is:\n",
    "\n",
    "[ClassName1, ClassName2, ...]\n",
    "\n",
    "The format of association list is:\n",
    "\n",
    "[(ClassName1, Cadinality1 ,ClassName2, Cardinality2), ...]\n",
    "\n",
    "for every class in class list write:\n",
    "\n",
    "class ClassName1{{}}\n",
    "class ClassName2{{}}\n",
    "\n",
    "for every tuple in association list write:\n",
    "\n",
    "ClassName1 \"Cardinality1\" -- \"Cardinality2\" Classname2.\n",
    "\n",
    "Output plantuml without futher text or explaination. Use tags \n",
    "@startuml\n",
    "at the beginning and \n",
    "@enduml \n",
    "at the end of the output.\n",
    "\n",
    "##############\n",
    "\n",
    "The class list is: \n",
    "\n",
    "{classes}\n",
    "\n",
    "#############\n",
    "\n",
    "##############\n",
    "\n",
    "The cardinality list is:\n",
    "\n",
    "{card}\n",
    "\n",
    "#############\n",
    "\n",
    "The plant uml is:\n",
    "                                    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cddce97-ff3f-446f-980f-0ffe83541fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83235d8f-368b-4d41-ac54-87b80b97784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47b7f80-a1cf-4720-9782-295d7d956d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbd394e-35fb-47dc-b14b-8790c25dd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chian_from_llms(llm):\n",
    "    prompt_class = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template = prompt_template_class                     \n",
    "    )\n",
    "    \n",
    "    prompt_assoc = PromptTemplate(\n",
    "        input_variables=[\"text\",\"classes\"],\n",
    "        template = prompt_template_assoc                    \n",
    "    )\n",
    "    \n",
    "    prompt_card = PromptTemplate(\n",
    "        input_variables=[\"text\",\"association\"],\n",
    "        template = prompt_template_card                  \n",
    "    )\n",
    "    \n",
    "    prompt_uml = PromptTemplate(\n",
    "        input_variables=[\"classes\",\"association\"],\n",
    "        template = prompt_template_plant                \n",
    "    )\n",
    "    \n",
    "    class_chain = prompt_class | llm | StrOutputParser()\n",
    "    assoc_chain = prompt_assoc | llm | StrOutputParser()\n",
    "    card_chain  =  prompt_card | llm | StrOutputParser()\n",
    "    uml_chain   =   prompt_uml | llm | StrOutputParser()\n",
    "    \n",
    "    full_chain = (\n",
    "         RunnableLambda(lambda x: {\"text\": x[\"text\"]})\n",
    "        | {\n",
    "            \"text\": lambda x: x[\"text\"],\n",
    "            \"classes\": class_chain\n",
    "        }\n",
    "        | {\n",
    "            \"text\": lambda x: x[\"text\"],\n",
    "            \"classes\": lambda x: x[\"classes\"],\n",
    "            \"association\": assoc_chain\n",
    "        }\n",
    "        | {\n",
    "            \"text\": lambda x: x[\"text\"],\n",
    "            \"association\": lambda x: x[\"association\"],\n",
    "            \"classes\": lambda x: x[\"classes\"],\n",
    "            \"card\": card_chain\n",
    "        }\n",
    "        | {\n",
    "            \"classes\": lambda x: x[\"classes\"],\n",
    "            \"card\": lambda x: x[\"card\"],\n",
    "            \"uml\": uml_chain\n",
    "        } \n",
    "        | RunnableLambda(lambda x: x[\"uml\"])\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return full_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4cd917-d184-4401-a0aa-2adab985a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "from time import sleep\n",
    "try:\n",
    "    import thread\n",
    "except ImportError:\n",
    "    import _thread as thread\n",
    "\n",
    "def quit_function(fn_name):\n",
    "    # print to stderr, unbuffered in Python 2.\n",
    "    print('{0} took too long'.format(fn_name), file=sys.stderr)\n",
    "    sys.stderr.flush() # Python 3 stderr is likely buffered.\n",
    "    thread.interrupt_main() # raises KeyboardInterrupt\n",
    "    \n",
    "def exit_after(s):\n",
    "    '''\n",
    "    use as decorator to exit process if \n",
    "    function takes longer than s seconds\n",
    "    '''\n",
    "    def outer(fn):\n",
    "        def inner(*args, **kwargs):\n",
    "            timer = threading.Timer(s, quit_function, args=[fn.__name__])\n",
    "            timer.start()\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            finally:\n",
    "                timer.cancel()\n",
    "            return result\n",
    "        return inner\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57cfbcd0-2bc0-4ba5-80b8-f14702706f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "#@exit_after(360)\n",
    "def run_chain(chain, text):\n",
    "    return chain.invoke({\"text\": text})\n",
    "    \n",
    "def process_subfolders_with_chain(root_folder_path, chain, type=''):\n",
    "    \"\"\"\n",
    "    Explores subfolders of the root folder (depth 1), processes each subfolder's `text.txt`\n",
    "    with the provided LangChain chain, and saves the result in a new file in the same folder.\n",
    "\n",
    "    Args:\n",
    "        root_folder_path (str): Path to the root folder.\n",
    "        chain: A LangChain chain instance to process text inputs.\n",
    "    \"\"\"\n",
    "    for subfolder_name in tqdm(os.listdir(root_folder_path)):\n",
    "        subfolder_path = os.path.join(root_folder_path, subfolder_name)\n",
    "        \n",
    "        # Ensure the current item is a subfolder\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            text_file_path = os.path.join(subfolder_path, \"text.txt\")\n",
    "            \n",
    "            # Check if `text.txt` exists in the subfolder\n",
    "            if not os.path.isfile(text_file_path):\n",
    "                # If `text.txt` is missing, check for any .docx file in the subfolder\n",
    "                docx_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".docx\")]\n",
    "                if docx_files:\n",
    "                    docx_file_path = os.path.join(subfolder_path, docx_files[0])\n",
    "                    # Extract content from the .docx file\n",
    "                    doc = Document(docx_file_path)\n",
    "                    text_content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "                    # Save the extracted content to `text.txt`\n",
    "                    with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                        text_file.write(text_content)\n",
    "\n",
    "            # Recheck if `text.txt` now exists\n",
    "            if os.path.isfile(text_file_path):\n",
    "                with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                with tracing_v2_enabled():\n",
    "                    # Call the LangChain chain with the input dictionary\n",
    "                    try:\n",
    "                        result = run_chain(chain, text)\n",
    "                    except:\n",
    "                        reuslt = \"\"\n",
    "\n",
    "                # Save the result to a new file in the same subfolder\n",
    "                result_file_path = os.path.join(subfolder_path, f\"result_cot_{type}.txt\")\n",
    "                with open(result_file_path, \"w\", encoding=\"utf-8\") as result_file:\n",
    "                    result_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc30d8ab-eae2-4d99-80d3-1e7bf7d5c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_result_txt_files(root_folder_path):\n",
    "    \"\"\"\n",
    "    Deletes every .txt file that starts with 'result_' in the subfolders of the root folder (depth 1).\n",
    "\n",
    "    Args:\n",
    "        root_folder_path (str): Path to the root folder.\n",
    "    \"\"\"\n",
    "    for subfolder_name in os.listdir(root_folder_path):\n",
    "        subfolder_path = os.path.join(root_folder_path, subfolder_name)\n",
    "        \n",
    "        # Ensure the current item is a subfolder\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                if file_name.startswith(\"result_\") and file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(subfolder_path, file_name)\n",
    "                    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5bf858-cd8f-4e3b-ae5b-629416320dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_result_txt_files(ROOT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a3d26-e23a-48a0-80c9-474784ed552c",
   "metadata": {},
   "source": [
    "# COT Open-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae9c99e-900e-4bf1-aba7-16fe824aceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bfad1f-1c94-44db-a15a-58afc3b780ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OPEN_AI = [\"o3-mini\",\"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828a51ad-9e16-4eae-8c83-761db2533177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.agents.chat.output_parser import ChatOutputParser\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd398db-4102-4faa-9313-c774c7d15f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_zero(model):\n",
    "    model_ = ChatOpenAI(model=model)\n",
    "    chain = build_chian_from_llms(model_)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6504bdc7-44b7-4aa6-bb6b-57acd88a6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_make_example(model):\n",
    "    model = ChatOpenAI(model=model)\n",
    "    chain = build_chian_from_llms(model)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0233047c-8846-42bf-a24a-40e7554d0990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "class Customer {}\n",
       "class Broker {}\n",
       "class InsurancePolicy {}\n",
       "class Offer {}\n",
       "class Contract {}\n",
       "class Invoice {}\n",
       "class ClaimCase {}\n",
       "class Estimator {}\n",
       "class Report {}\n",
       "class CompensationDecision {}\n",
       "class Payment {}\n",
       "\n",
       "Customer \"1..1\" -- \"0..*\" Broker\n",
       "Customer \"1..*\" -- \"0..*\" InsurancePolicy\n",
       "Customer \"0..*\" -- \"1..1\" Offer\n",
       "Customer \"0..*\" -- \"1..1\" Contract\n",
       "Customer \"0..*\" -- \"1..1\" Invoice\n",
       "Customer \"0..*\" -- \"1..1\" ClaimCase\n",
       "ClaimCase \"1..*\" -- \"0..*\" Estimator\n",
       "Estimator \"0..*\" -- \"1..1\" Report\n",
       "ClaimCase \"1..*\" -- \"1..1\" Report\n",
       "ClaimCase \"0..1\" -- \"1..1\" CompensationDecision\n",
       "Customer \"0..*\" -- \"1..1\" Payment\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(open_ai_make_example(MODEL_OPEN_AI[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51828f70-344d-4e2a-b442-4a4bd158f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with o3-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [29:02<00:00, 36.30s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_OPEN_AI[0:1]:\n",
    "    print(f\"COT with {model}\")\n",
    "    open_ai_zero(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878307c-cf11-4ff0-9a16-2ca8b6ea7a72",
   "metadata": {},
   "source": [
    "# COT Open LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a07202-45d1-40b5-8985-fcb10cb05c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7c96a-0e98-4aac-9fec-b60f0c1e588e",
   "metadata": {},
   "source": [
    "## Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e16583e2-6c85-4ff3-94af-708c624d61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af478a3-2ad1-46eb-b7a4-52a0bbab4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DEEPSEEK = [\"deepseek-chat\"] #, \"deepseek-reasoner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10240f9b-7cb9-4245-9417-90f92bcdfd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_make_example(model):\n",
    "    model = llm = ChatDeepSeek(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            # api_key=\"...\",\n",
    "            # other params...\n",
    "        )\n",
    "    chain = build_chian_from_llms(model)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04deece-632f-4dec-90e0-38ac1d1b8d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_cot(model):\n",
    "    model_ = ChatDeepSeek(model=model)\n",
    "    chain = build_chian_from_llms(model_)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03a08fe-0df2-4b45-9395-0485d35b6f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```plantuml\n",
       "@startuml\n",
       "class InsuranceCompany{}\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class InsurancePolicy{}\n",
       "class Contract{}\n",
       "class Claim{}\n",
       "class ClaimCase{}\n",
       "class Estimator{}\n",
       "class CompensationDecision{}\n",
       "class Invoice{}\n",
       "\n",
       "InsuranceCompany \"1..1\" -- \"0..*\" Customer\n",
       "Customer \"1..1\" -- \"1..1\" Broker\n",
       "Customer \"1..*\" -- \"0..*\" InsurancePolicy\n",
       "Broker \"1..1\" -- \"0..*\" InsurancePolicy\n",
       "InsurancePolicy \"1..1\" -- \"1..1\" Contract\n",
       "Customer \"1..*\" -- \"0..*\" Claim\n",
       "Claim \"1..1\" -- \"1..*\" ClaimCase\n",
       "ClaimCase \"1..1\" -- \"1..*\" Estimator\n",
       "ClaimCase \"1..1\" -- \"1..1\" CompensationDecision\n",
       "Contract \"1..1\" -- \"1..*\" Invoice\n",
       "@enduml\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 254 ms, sys: 48 ms, total: 302 ms\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "display_markdown(deepseek_make_example(MODEL_DEEPSEEK[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a5e8977-d865-44e2-a73c-fe144817e0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT shot with deepseek-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [31:46<00:00, 39.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_DEEPSEEK:\n",
    "    print(f\"COT with {model}\")\n",
    "    deepseek_cot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427df250-444c-4550-a8f8-1e7f95654880",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4cea3db-26fc-441b-819b-239eb68b5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b346e67f-ce7f-4cf8-b316-9c917b114ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ANTHROPIC = [\"claude-3-7-sonnet-20250219\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cbc5958-af4c-4327-985c-67ec6ef9d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_make_example(model):\n",
    "    model = ChatAnthropic(model=model,temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,)\n",
    "    chain = build_chian_from_llms(model)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dfd5e81-b935-44a9-9dc2-92d7ad49eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_cot(model):\n",
    "    model_ = ChatAnthropic(model=model)\n",
    "    chain = build_chian_from_llms(model_)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "910658ec-e5fd-4354-b6eb-3a4eeaadb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class InsurancePolicy{}\n",
       "class Contract{}\n",
       "class Offer{}\n",
       "class Invoice{}\n",
       "class Claim{}\n",
       "class ClaimCase{}\n",
       "class Estimator{}\n",
       "class Report{}\n",
       "class CompensationDecision{}\n",
       "class Payment{}\n",
       "\n",
       "Customer \"1..1\" -- \"1..*\" Broker\n",
       "Customer \"1..*\" -- \"0..*\" InsurancePolicy\n",
       "Customer \"1..*\" -- \"1..*\" Contract\n",
       "Customer \"1..*\" -- \"0..*\" Offer\n",
       "Customer \"1..*\" -- \"0..*\" Invoice\n",
       "Customer \"1..*\" -- \"0..*\" Claim\n",
       "Customer \"1..*\" -- \"0..*\" Payment\n",
       "Broker \"1..*\" -- \"0..*\" Contract\n",
       "Broker \"1..*\" -- \"0..*\" Offer\n",
       "InsurancePolicy \"1..1\" -- \"1..1\" Contract\n",
       "InsurancePolicy \"1..1\" -- \"0..*\" Invoice\n",
       "Contract \"1..1\" -- \"0..1\" Offer\n",
       "Claim \"1..1\" -- \"1..*\" ClaimCase\n",
       "ClaimCase \"1..*\" -- \"1..*\" Estimator\n",
       "ClaimCase \"1..1\" -- \"0..1\" CompensationDecision\n",
       "Estimator \"1..*\" -- \"1..*\" Report\n",
       "CompensationDecision \"1..1\" -- \"0..1\" Payment\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(anthropic_make_example(MODEL_ANTHROPIC[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4440c530-0a45-413d-ac43-83a31f48062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT shot with claude-3-7-sonnet-20250219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [04:52<00:00,  6.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_ANTHROPIC:\n",
    "    print(f\"COT with {model}\")\n",
    "    anthropic_cot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30f2b7-0eb2-465a-9224-98a42a171fe0",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848047d5-35d6-4272-9ba4-a1e6a39346bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ce832ba-b152-4775-87cd-c69565f67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OLLAMA = [] #[\"llama3.2:3b-text-fp16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df4335f3-03a9-45af-8d82-bba70439807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_zero(model):\n",
    "    model_ = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "    )\n",
    "    chain = build_chian_from_llms(model_)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0477e649-cfda-4654-a52c-c58940935865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_make_example(model):\n",
    "    model = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "    )\n",
    "    chain = build_chian_from_llms(model)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33afa71e-b8c6-4c49-b11b-b89cfb704409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_markdown(ollama_make_example(MODEL_OLLAMA[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48b60c86-7d08-4790-b5ae-b875c102788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODEL_OLLAMA:\n",
    "    print(f\"COT with {model}\")\n",
    "    ollama_zero(model)\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db448981-cf0a-42b5-b7ed-cb73bf0fbb56",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96994dbd-a971-4ae7-9ee9-5d0705d9a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9DBE5D5C-AC87-30CA-96DA-F5BC116EDA2B> /Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A51C8C05-245A-3989-8D3C-9A6704422CA5> /Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace, HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b07eead8-de14-4dc8-95bf-02c77101f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HUGGINGFACE = [] #[\"google/gemma-2-2b-it\"] #[\"Qwen/Qwen2.5-3B-Instruct\", \"microsoft/Phi-3-mini-4k-instruct\", \"google/gemma-2-27b-it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccce77e7-20f7-45b0-817b-b04107541a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_cot(model):\n",
    "    model_id = model\n",
    "    \n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=model_id,\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        temperature=0.01,\n",
    ")\n",
    "\n",
    "    chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    chain = build_chian_from_llms(chat)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model_id.replace(\"/\",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8cab307-e878-41a6-a13d-0f6904a4f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_make_example(model):\n",
    "\n",
    "    model_id = model\n",
    "    \n",
    "    \n",
    "\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=model_id,\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1024}\n",
    "    )\n",
    "\n",
    "    chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    chain = build_chian_from_llms(chat)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "        As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "        \n",
    "        In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "        \n",
    "        \"\"\"})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cfe2884-9b34-4874-ae95-9985e4a09870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_markdown(huggingface_make_example(MODEL_HUGGINGFACE[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba9b628f-024e-450a-8d85-2db5b0c67c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODEL_HUGGINGFACE:\n",
    "    print(f\"COT with {model}\")\n",
    "    huggingface_zero(model)\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f5762-513f-4c2d-93f8-10598a2e7827",
   "metadata": {},
   "source": [
    "## Mlx-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a61c5ea0-0f4a-4cc2-8220-0d38d8f91bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import MLXPipeline\n",
    "from langchain_community.chat_models import ChatMLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c805336-ba69-459f-9fd9-3271aeec6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MLX = [\"mlx-community/phi-4-8bit\",\n",
    "             \"mlx-community/Falcon3-10B-Instruct-8bit\", \n",
    "             \"mlx-community/Qwen2.5-14B-Instruct-4bit\",\n",
    "             \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\",\n",
    "             \"mlx-community/DeepSeek-R1-Distill-Qwen-7B-8bit\",\n",
    "             \"mlx-community/Llama-3.2-3B-Instruct\",\n",
    "             \"mlx-community/gemma-2-9b-8bit\",\n",
    "             #\"mlx-community/gemma-2-27b-it-4bit\",\n",
    "            # \"mlx-community/Mamba-Codestral-7B-v0.1-8bit\",\n",
    "            #\"mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "958151bc-0e7e-418a-9127-98d1e00e558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_zero(model):\n",
    "    llm = MLXPipeline.from_model_id(\n",
    "        model_id=model,\n",
    "        pipeline_kwargs={\"max_tokens\": 30_000, \"temp\": 0.1},\n",
    "    )\n",
    "    chat = ChatMLX(llm=llm)\n",
    "    chain = build_chian_from_llms(chat)\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model.replace(\"/\",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68a8ee38-7b7b-4a1f-8dec-02cb504b53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_make_example(model):\n",
    "    llm = MLXPipeline.from_model_id(\n",
    "        model_id=model,\n",
    "        pipeline_kwargs={\"max_tokens\": 3000, \"temp\": 0.7},\n",
    "    )\n",
    "    chat = ChatMLX(llm=llm)\n",
    "    chain = build_chian_from_llms(chat)\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30a9e5a1-d9a8-447f-877e-645a04c5d2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a56b4ea486a4d94b8b65c6a44ef100c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "class InsurancePolicy{}\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class Contract{}\n",
       "class Claim{}\n",
       "class Estimator{}\n",
       "class Compensation{}\n",
       "\n",
       "Customer \"0..1\" -- \"1..*\" Broker\n",
       "Customer \"0..*\" -- \"1..*\" Contract\n",
       "Customer \"0..*\" -- \"1..*\" Claim\n",
       "Broker \"0..*\" -- \"0..*\" Contract\n",
       "Estimator \"0..*\" -- \"0..*\" Claim\n",
       "Claim \"1..1\" -- \"0..*\" Compensation\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(mlx_make_example(MODEL_MLX[2]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccea0873-879d-4fcb-a1a0-d97197370751",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d835f1b-7493-4a1b-8c51-26142777d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/phi-4-8bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a86317d0eda4f2e82db79ae2b73bdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 44/44 [25:02<00:00, 34.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/Falcon3-10B-Instruct-8bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57f1dfc63f4f19ae0e73d7e3593a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 44/44 [16:38<00:00, 22.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/Qwen2.5-14B-Instruct-4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd7a677db0e472bb15d89a277a2b448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 44/44 [11:26<00:00, 15.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/Mistral-7B-Instruct-v0.3-4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41f9350a67a41d4a4f3c05067f86a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 44/44 [09:18<00:00, 12.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/DeepSeek-R1-Distill-Qwen-7B-8bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e596409cf34a07a3be2af9b031499d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▉                                        | 1/44 [07:00<5:01:17, 420.40s/it]Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x3c30c8ad0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 5354\n",
      "API Key: lsv2_********************************************4b\n",
      "  5%|█▊                                      | 2/44 [28:46<10:59:02, 941.49s/it]Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x4580eafd0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 7009\n",
      "API Key: lsv2_********************************************4b\n",
      "Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x44e44b250>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'))\n",
      "Content-Length: 6184\n",
      "API Key: lsv2_********************************************4b\n",
      "100%|████████████████████████████████████████| 44/44 [1:48:27<00:00, 147.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COT with mlx-community/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 44/44 [23:21<00:00, 31.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_MLX:\n",
    "    print(f\"COT with {model}\")\n",
    "    mlx_zero(model)\n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067e756-9fdc-4a62-9aef-70ac6df19f12",
   "metadata": {},
   "source": [
    "### Clean Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3904bc0-b746-400e-b5f0-4988b10d459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
