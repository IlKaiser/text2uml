{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1354cc70-f0e5-4ceb-8abb-e8290f9e4a05",
   "metadata": {},
   "source": [
    "# Template Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48051682-039d-4dea-80fa-416f0e56d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_FOLDER = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abf5dda-e975-442e-a7c1-58657f77f816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Step 1. Class\\n    Step 2. Association\\n    Step 3. Cardinaliy one by one'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"\"\"You are a helpful assistant expert of conceptual modeling, information extraction and UML modeling.\n",
    "                  \n",
    "                  You will be asked by the user to create a plant UMl model from specification text. Do so in the most\n",
    "                  clear way possible, avoid class properties and assign molteplicity. \n",
    "\n",
    "                  Do not include attributes for classes. For example the class Book would be:\n",
    "\n",
    "                  class Book{{}}\n",
    "\n",
    "                  Use only bi-directional arc for relations and no description. For example a relation between\n",
    "                  the class Book and the class Page, if the Book can have from one to many pages and the \n",
    "                  pages could have exactly one book, would be:\n",
    "\n",
    "                  Book \"1..1\" -- \"1..*\" Page\n",
    "\n",
    "                  Adapt the cardinality to each case. If the cardinality would be \"0..*\", the default one, omit it.\n",
    "\n",
    "                  The plantuml has to be the class diagram. In generating the diagram perform this steps in order\n",
    "\n",
    "                  1. Extract class from text\n",
    "                  2. Extract relations form text\n",
    "                  3. Assign the relation to the corresponding class\n",
    "                  4. Add cardinality to the relations\n",
    "\n",
    "                  Put everything in this order: first all classes and then all relations. In our example would be:\n",
    "\n",
    "                  class Book{{}}\n",
    "                  class Page{{}}\n",
    "\n",
    "                  Book \"1..1\" -- \"1..*\" Page\n",
    "\n",
    "\n",
    "                  Output plantuml without futher text or explaination.\n",
    "                  \"\"\"),\n",
    "    (\"user\", \"Transform into plant uml this specification text: {text}\")\n",
    "])\n",
    "\n",
    "\n",
    "\"\"\" Step 1. Class\n",
    "    Step 2. Association\n",
    "    Step 3. Cardinaliy one by one\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08b31a1d-21e0-4367-b55b-bc9b97164bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You will be asked by the user to create a plant UMl model from specification text. Do so in the most\n",
    "clear way possible, avoid class properties and assign molteplicity. \n",
    "\n",
    "Do not include attributes for classes. For example the class Book would be:\n",
    "\n",
    "class Book{{}}\n",
    "\n",
    "Use only bi-directional arc for relations and no description. For example a relation between\n",
    "the class Book and the class Page, if the Book can have from one to many pages and the \n",
    "pages could have exactly one book, would be:\n",
    "\n",
    "Book \"1..1\" -- \"1..*\" Page\n",
    "\n",
    "Adapt the cardinality to each case.\n",
    "\n",
    "The plantuml has to be the class diagram. In generating the diagram perform this steps in order\n",
    "\n",
    "1. Extract class from text\n",
    "2. Extract relations form text\n",
    "3. Assign the relation to the corresponding class\n",
    "4. Add cardinality to the relations\n",
    "\n",
    "Put everything in this order: first all classes and then all relations. In our example would be:\n",
    "\n",
    "@startuml\n",
    "\n",
    "class Book{{}}\n",
    "class Page{{}}\n",
    "\n",
    "Book \"1..1\" -- \"1..*\" Page\n",
    "\n",
    "@enduml\n",
    "\n",
    "Output plantuml without futher text or explaination.\n",
    "\n",
    "##############\n",
    "\n",
    "The specification text is:\n",
    "\n",
    "{text}\n",
    "\n",
    "##############\n",
    "\n",
    "The uml output is:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52bfe56-9d50-4bd6-baae-0aeee67212fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "from time import sleep\n",
    "try:\n",
    "    import thread\n",
    "except ImportError:\n",
    "    import _thread as thread\n",
    "\n",
    "def quit_function(fn_name):\n",
    "    # print to stderr, unbuffered in Python 2.\n",
    "    print('{0} took too long'.format(fn_name), file=sys.stderr)\n",
    "    sys.stderr.flush() # Python 3 stderr is likely buffered.\n",
    "    thread.interrupt_main() # raises KeyboardInterrupt\n",
    "    \n",
    "def exit_after(s):\n",
    "    '''\n",
    "    use as decorator to exit process if \n",
    "    function takes longer than s seconds\n",
    "    '''\n",
    "    def outer(fn):\n",
    "        def inner(*args, **kwargs):\n",
    "            timer = threading.Timer(s, quit_function, args=[fn.__name__])\n",
    "            timer.start()\n",
    "            try:\n",
    "                result = fn(*args, **kwargs)\n",
    "            finally:\n",
    "                timer.cancel()\n",
    "            return result\n",
    "        return inner\n",
    "    return outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57cfbcd0-2bc0-4ba5-80b8-f14702706f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "@exit_after(360)\n",
    "def run_chain(chain, text):\n",
    "    return chain.invoke({\"text\": text})\n",
    "\n",
    "def process_subfolders_with_chain(root_folder_path, chain, type=''):\n",
    "    \"\"\"\n",
    "    Explores subfolders of the root folder (depth 1), processes each subfolder's `text.txt`\n",
    "    with the provided LangChain chain, and saves the result in a new file in the same folder.\n",
    "\n",
    "    Args:\n",
    "        root_folder_path (str): Path to the root folder.\n",
    "        chain: A LangChain chain instance to process text inputs.\n",
    "    \"\"\"\n",
    "    for subfolder_name in tqdm(os.listdir(root_folder_path)):\n",
    "        subfolder_path = os.path.join(root_folder_path, subfolder_name)\n",
    "        \n",
    "        # Ensure the current item is a subfolder\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            text_file_path = os.path.join(subfolder_path, \"text.txt\")\n",
    "            \n",
    "            # Check if `text.txt` exists in the subfolder\n",
    "            if not os.path.isfile(text_file_path):\n",
    "                # If `text.txt` is missing, check for any .docx file in the subfolder\n",
    "                docx_files = [f for f in os.listdir(subfolder_path) if f.endswith(\".docx\")]\n",
    "                if docx_files:\n",
    "                    docx_file_path = os.path.join(subfolder_path, docx_files[0])\n",
    "                    # Extract content from the .docx file\n",
    "                    doc = Document(docx_file_path)\n",
    "                    text_content = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "\n",
    "                    # Save the extracted content to `text.txt`\n",
    "                    with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "                        text_file.write(text_content)\n",
    "\n",
    "            # Recheck if `text.txt` now exists\n",
    "            if os.path.isfile(text_file_path):\n",
    "                with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                    text = file.read()\n",
    "\n",
    "                with tracing_v2_enabled():\n",
    "                    # Call the LangChain chain with the input dictionary\n",
    "                    try:\n",
    "                        result = run_chain(chain, text)\n",
    "                    except:\n",
    "                        result = \"\"\n",
    "\n",
    "                # Save the result to a new file in the same subfolder\n",
    "                result_file_path = os.path.join(subfolder_path, f\"result_{type}.txt\")\n",
    "                with open(result_file_path, \"w\", encoding=\"utf-8\") as result_file:\n",
    "                    result_file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc30d8ab-eae2-4d99-80d3-1e7bf7d5c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_result_txt_files(root_folder_path):\n",
    "    \"\"\"\n",
    "    Deletes every .txt file that starts with 'result_' in the subfolders of the root folder (depth 1).\n",
    "\n",
    "    Args:\n",
    "        root_folder_path (str): Path to the root folder.\n",
    "    \"\"\"\n",
    "    for subfolder_name in os.listdir(root_folder_path):\n",
    "        subfolder_path = os.path.join(root_folder_path, subfolder_name)\n",
    "        \n",
    "        # Ensure the current item is a subfolder\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file_name in os.listdir(subfolder_path):\n",
    "                if \"mlx-community_Llama-3.2-3B-Instruct-4bit\" in file_name and file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(subfolder_path, file_name)\n",
    "                    os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5bf858-cd8f-4e3b-ae5b-629416320dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_result_txt_files(ROOT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a3d26-e23a-48a0-80c9-474784ed552c",
   "metadata": {},
   "source": [
    "# Zero Shot Open-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cdf8890-f85b-45aa-81c5-44aa6054305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9bfad1f-1c94-44db-a15a-58afc3b780ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OPEN_AI = [\"o3-mini\", \"gpt-4o-mini\", \"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828a51ad-9e16-4eae-8c83-761db2533177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd398db-4102-4faa-9313-c774c7d15f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_zero(model):\n",
    "    model_ = ChatOpenAI(model=model)\n",
    "    chain = prompt_template | model_ | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6504bdc7-44b7-4aa6-bb6b-57acd88a6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ai_make_example(model):\n",
    "    model = ChatOpenAI(model=model)\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0233047c-8846-42bf-a24a-40e7554d0990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "\n",
       "class InsuranceCompany{}\n",
       "class Customer{}\n",
       "class InsurancePolicy{}\n",
       "class Broker{}\n",
       "class HeadOffice{}\n",
       "class AccountManager{}\n",
       "class HelpDesk{}\n",
       "class Offer{}\n",
       "class Contract{}\n",
       "class Invoice{}\n",
       "class ClaimCase{}\n",
       "class Estimator{}\n",
       "class Report{}\n",
       "class CompensationDecision{}\n",
       "class Payment{}\n",
       "\n",
       "InsuranceCompany \"1..1\" -- \"0..*\" Customer\n",
       "InsuranceCompany \"1..1\" -- \"1..*\" InsurancePolicy\n",
       "Customer \"1..1\" -- \"1..1\" Broker\n",
       "HeadOffice \"1..1\" -- \"0..*\" Broker\n",
       "Customer \"1..1\" -- \"1..1\" AccountManager\n",
       "HelpDesk \"1..1\" -- \"1..1\" AccountManager\n",
       "Customer \"1..1\" -- \"0..1\" Offer\n",
       "Offer \"1..1\" -- \"0..1\" Contract\n",
       "Contract \"1..1\" -- \"1..1\" AccountManager\n",
       "Customer \"1..1\" -- \"0..*\" Invoice\n",
       "Customer \"1..1\" -- \"0..*\" ClaimCase\n",
       "InsuranceCompany \"1..1\" -- \"0..*\" ClaimCase\n",
       "ClaimCase \"1..1\" -- \"1..*\" Estimator\n",
       "Estimator \"1..1\" -- \"1..*\" Report\n",
       "ClaimCase \"1..1\" -- \"0..1\" CompensationDecision\n",
       "CompensationDecision \"1..1\" -- \"1..1\" Payment\n",
       "Customer \"1..1\" -- \"0..1\" Payment\n",
       "\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(open_ai_make_example(MODEL_OPEN_AI[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51828f70-344d-4e2a-b442-4a4bd158f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot with gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [00:55<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_OPEN_AI[1:2]:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    open_ai_zero(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878307c-cf11-4ff0-9a16-2ca8b6ea7a72",
   "metadata": {},
   "source": [
    "# Zero Shot Open LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a07202-45d1-40b5-8985-fcb10cb05c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf90eab-ebe3-4e31-8459-9a29e13c4199",
   "metadata": {},
   "source": [
    "## Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a48b979c-f2f7-4823-b270-85f6314019b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_deepseek import ChatDeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6096d388-e6d4-498b-92d7-389841396d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DEEPSEEK = [\"deepseek-chat\"]#, \"deepseek-reasoner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4db5fab-11c5-42ce-9b27-017b3109dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_make_example(model):\n",
    "    model = llm = ChatDeepSeek(\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            # api_key=\"...\",\n",
    "            # other params...\n",
    "        )\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae8650e9-1756-4106-ab51-8abe46a0436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepseek_zero(model):\n",
    "    model_ = ChatDeepSeek(model=model)\n",
    "    chain = prompt_template | model_ | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "241e63e2-4de9-4fe0-b30e-f4e0df36a6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "\n",
       "class AlphaInsurance{}\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class InsurancePolicy{}\n",
       "class Contract{}\n",
       "class Offer{}\n",
       "class Claim{}\n",
       "class ClaimCase{}\n",
       "class Estimator{}\n",
       "class CompensationDecision{}\n",
       "class Report{}\n",
       "\n",
       "AlphaInsurance \"1..1\" -- \"1..*\" Customer\n",
       "Customer \"1..1\" -- \"1..1\" Broker\n",
       "Customer \"1..1\" -- \"0..*\" InsurancePolicy\n",
       "Broker \"1..1\" -- \"0..*\" InsurancePolicy\n",
       "InsurancePolicy \"1..1\" -- \"1..1\" Contract\n",
       "Contract \"1..1\" -- \"0..*\" Offer\n",
       "Customer \"1..1\" -- \"0..*\" Claim\n",
       "Claim \"1..1\" -- \"1..*\" ClaimCase\n",
       "ClaimCase \"1..1\" -- \"1..*\" Estimator\n",
       "ClaimCase \"1..1\" -- \"1..1\" CompensationDecision\n",
       "Estimator \"1..1\" -- \"1..*\" Report\n",
       "\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.4 ms, sys: 14.3 ms, total: 110 ms\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "display_markdown(deepseek_make_example(MODEL_DEEPSEEK[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43f62ba9-f2e5-4abb-af4c-0ca28394ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot with deepseek-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [08:40<00:00, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot with deepseek-reasoner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run_chain took too long                       | 6/48 [12:21<1:26:04, 122.96s/it]\n",
      "run_chain took too long                       | 8/48 [18:21<1:39:12, 148.82s/it]\n",
      "run_chain took too long                       | 9/48 [24:22<2:10:18, 200.47s/it]\n",
      "run_chain took too long                      | 11/48 [30:22<1:58:29, 192.14s/it]\n",
      "run_chain took too long                      | 14/48 [44:57<2:17:39, 242.93s/it]\n",
      "run_chain took too long                      | 17/48 [54:11<1:49:02, 211.06s/it]\n",
      "run_chain took too long                    | 22/48 [1:10:49<1:26:27, 199.51s/it]\n",
      "run_chain took too long█                   | 24/48 [1:22:09<1:41:46, 254.44s/it]\n",
      "run_chain took too long█▊                  | 25/48 [1:28:09<1:47:48, 281.23s/it]\n",
      "run_chain took too long██▌                 | 26/48 [1:34:10<1:50:49, 302.24s/it]\n",
      "run_chain took too long████▉               | 29/48 [1:40:11<1:05:03, 205.46s/it]\n",
      "run_chain took too long█████▊              | 30/48 [1:46:11<1:11:02, 236.79s/it]\n",
      "run_chain took too long██████▌             | 31/48 [1:52:11<1:14:56, 264.49s/it]\n",
      "run_chain took too long███████▎            | 32/48 [1:58:12<1:16:45, 287.81s/it]\n",
      "run_chain took too long██████████▎           | 34/48 [2:04:13<56:52, 243.78s/it]\n",
      "run_chain took too long████████████▊         | 37/48 [2:15:05<37:59, 207.26s/it]\n",
      "run_chain took too long█████████████▋        | 38/48 [2:21:05<40:08, 240.80s/it]\n",
      "run_chain took too long█████████████████▊    | 43/48 [2:31:17<10:57, 131.43s/it]\n",
      "100%|████████████████████████████████████████| 48/48 [2:42:07<00:00, 202.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_DEEPSEEK:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    deepseek_zero(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36807c7-4c79-4b9a-9979-59a3c39876e2",
   "metadata": {},
   "source": [
    "## Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbfcb77f-4070-4821-8e8c-f730d093d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed374282-60d1-4cbe-aec0-18c71398092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ANTHROPIC = [\"claude-3-7-sonnet-20250219\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7af5e695-ddc7-426d-bb4c-2f2d614318e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_make_example(model):\n",
    "    model = ChatAnthropic(model=model,temperature=0,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,)\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0961d93-e307-4554-8b52-3cbf53ce06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_zero(model):\n",
    "    model_ = ChatAnthropic(model=model)\n",
    "    chain = prompt_template | model_ | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "021525ac-3b79-45d2-9889-4ce34f7affc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "@startuml\n",
       "\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class InsurancePolicy{}\n",
       "class Contract{}\n",
       "class Offer{}\n",
       "class Invoice{}\n",
       "class Claim{}\n",
       "class ClaimCase{}\n",
       "class Estimator{}\n",
       "class CompensationDecision{}\n",
       "class Payment{}\n",
       "class Report{}\n",
       "\n",
       "Customer \"1..*\" -- \"1..1\" Broker\n",
       "Customer \"1..*\" -- \"0..*\" InsurancePolicy\n",
       "Customer \"1..*\" -- \"0..*\" Contract\n",
       "Customer \"1..*\" -- \"0..*\" Offer\n",
       "Customer \"1..*\" -- \"0..*\" Invoice\n",
       "Customer \"1..1\" -- \"0..*\" Claim\n",
       "Claim \"1..1\" -- \"1..*\" ClaimCase\n",
       "ClaimCase \"1..*\" -- \"1..*\" Estimator\n",
       "Estimator \"1..*\" -- \"1..*\" Report\n",
       "ClaimCase \"1..1\" -- \"0..1\" CompensationDecision\n",
       "CompensationDecision \"1..1\" -- \"0..1\" Payment\n",
       "Contract \"1..1\" -- \"1..*\" InsurancePolicy\n",
       "\n",
       "@enduml\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(anthropic_make_example(MODEL_ANTHROPIC[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d675a446-cf6e-4316-a79d-628c6b10b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot with claude-3-7-sonnet-20250219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [01:47<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_ANTHROPIC:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    anthropic_zero(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30f2b7-0eb2-465a-9224-98a42a171fe0",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "848047d5-35d6-4272-9ba4-a1e6a39346bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce832ba-b152-4775-87cd-c69565f67b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OLLAMA = [] #[\"llama3.2:3b-text-fp16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4335f3-03a9-45af-8d82-bba70439807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_zero(model):\n",
    "    model_ = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        timeout = 3,\n",
    "    )\n",
    "    chain = prompt_template | model_ | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0477e649-cfda-4654-a52c-c58940935865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_make_example(model):\n",
    "    model = ChatOllama(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        timeout = 3,\n",
    "    )\n",
    "    chain = prompt_template | model | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33afa71e-b8c6-4c49-b11b-b89cfb704409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_markdown(ollama_make_example(MODEL_OLLAMA[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48b60c86-7d08-4790-b5ae-b875c102788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODEL_OLLAMA:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    ollama_zero(model)\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db448981-cf0a-42b5-b7ed-cb73bf0fbb56",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96994dbd-a971-4ae7-9ee9-5d0705d9a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <9DBE5D5C-AC87-30CA-96DA-F5BC116EDA2B> /Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <A51C8C05-245A-3989-8D3C-9A6704422CA5> /Users/marcocalamo/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace, HuggingFaceEndpoint\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b07eead8-de14-4dc8-95bf-02c77101f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HUGGINGFACE = [] #[\"Qwen/Qwen2.5-3B-Instruct\", \"microsoft/Phi-3-mini-4k-instruct\", \"google/gemma-2-27b-it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccce77e7-20f7-45b0-817b-b04107541a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_zero(model):\n",
    "    model_id = model\n",
    "    \n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=model_id,\n",
    "        task=\"text-generation\",\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.03,\n",
    "        temperature=0.01,\n",
    "    )\n",
    "\n",
    "    chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    chain = prompt_template | chat | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model_id.replace(\"/\",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8cab307-e878-41a6-a13d-0f6904a4f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_make_example(model):\n",
    "\n",
    "    model_id = model\n",
    "    \n",
    "    \n",
    "\n",
    "    llm = HuggingFacePipeline.from_model_id(\n",
    "        model_id=model_id,\n",
    "        task=\"text-generation\",\n",
    "        pipeline_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1024}\n",
    "    )\n",
    "\n",
    "    chat = ChatHuggingFace(llm=llm, verbose=True)\n",
    "    \n",
    "    chain = prompt_template | chat | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "        As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "        \n",
    "        In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "        \n",
    "        \"\"\"})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cfe2884-9b34-4874-ae95-9985e4a09870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display_markdown(huggingface_make_example(MODEL_HUGGINGFACE[0]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9b628f-024e-450a-8d85-2db5b0c67c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODEL_HUGGINGFACE:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    huggingface_zero(model)\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f5762-513f-4c2d-93f8-10598a2e7827",
   "metadata": {},
   "source": [
    "## Mlx-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61c5ea0-0f4a-4cc2-8220-0d38d8f91bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import MLXPipeline\n",
    "from langchain_community.chat_models import ChatMLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c805336-ba69-459f-9fd9-3271aeec6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MLX = [\"mlx-community/phi-4-8bit\",\n",
    "             \"mlx-community/Falcon3-10B-Instruct-8bit\", \n",
    "             \"mlx-community/Qwen2.5-14B-Instruct-4bit\",\n",
    "             \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\",\n",
    "             \"mlx-community/DeepSeek-R1-Distill-Qwen-7B-8bit\",\n",
    "             \"mlx-community/Llama-3.2-3B-Instruct\",\n",
    "             \"mlx-community/gemma-2-9b-8bit\",\n",
    "             #\"mlx-community/gemma-2-27b-it-4bit\",\n",
    "            # \"mlx-community/Mamba-Codestral-7B-v0.1-8bit\",\n",
    "            #\"mlx-community/CodeLlama-13b-Instruct-hf-4bit-MLX\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "958151bc-0e7e-418a-9127-98d1e00e558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_zero(model):\n",
    "    llm = MLXPipeline.from_model_id(\n",
    "        model_id=model,\n",
    "        pipeline_kwargs={\"max_tokens\": 15_000, \"temp\": 0.1},\n",
    "    )\n",
    "    chat = ChatMLX(llm=llm)\n",
    "    chain = prompt_template | chat | StrOutputParser()\n",
    "    process_subfolders_with_chain(ROOT_FOLDER, chain, type=model.replace(\"/\",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a8ee38-7b7b-4a1f-8dec-02cb504b53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_make_example(model):\n",
    "    llm = MLXPipeline.from_model_id(\n",
    "        model_id=model,\n",
    "        pipeline_kwargs={\"max_tokens\": 2000, \"temp\": 0.7},\n",
    "    )\n",
    "    chat = ChatMLX(llm=llm)\n",
    "    chain = prompt_template | chat | StrOutputParser()\n",
    "    res = chain.invoke({\"text\": \"\"\"Alpha Insurance is an insurance company that provides its clients with various types of insurance policies. \n",
    "As soon as a customer addresses Alpha Insurance, a broker is assigned to follow the customer’s file. The broker is registered in the system, so that when a customer calls, based on the contract, the help desk can immediately trace who is the customer's first account manager. After the broker is assigned to the customer, the latter indicates which type(s) of insurance policy they would like to sign for, so the broker could, depending on the case, either assess the customer’s profile on the spot or send the customer’s file for analysis to the head office. After the customer’s profile has been assessed and the customer has been deemed trustworthy , a preliminary contract/offer on an insurance product is made to the customer either in person or by email. (Such offers can also be extended to already existing customers.) If the customer agrees to the offer, the contract is signed by both parties. After the signing of the contract, the client enjoys the coverage and is invoiced (monthly or yearly – depending on the choice made in the contract) according to the price of the insurance product they bought.\n",
    "\n",
    "In case the insured event happens, a customer should send a claim for compensation. Then the company opens one or several claim cases (e.g. in case of an accident, often material damage & physical damage are handled separately). Once the case file is complete, it is sent for assessment by different estimators based on their area of expertise. According to the reports issued by the estimators, it is decided whether the claim case is approved. In case of approval the compensation decision is registered that stipulates which costs are eligible for (partial) refund.  For the supplied documents, the sum of compensation is calculated and the compensation is paid to the customer’s account.  The estimators’ reports must be stored in the database for at least one year after the payment of compensation for legal purposes. \n",
    "\n",
    "\"\"\"})\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a9e5a1-d9a8-447f-877e-645a04c5d2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55267198c94c79973f4a0822f17bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "@startuml\n",
       "\n",
       "class Customer{}\n",
       "class Broker{}\n",
       "class Contract{}\n",
       "class InsurancePolicy{}\n",
       "class Claim{}\n",
       "class Estimator{}\n",
       "class CompensationDecision{}\n",
       "class Document{}\n",
       "\n",
       "Customer -- Broker\n",
       "Customer -- Contract\n",
       "Contract -- InsurancePolicy\n",
       "Customer -- Claim\n",
       "Claim -- Estimator\n",
       "Claim -- CompensationDecision\n",
       "CompensationDecision -- Document\n",
       "\n",
       "@enduml"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(mlx_make_example(MODEL_MLX[2]), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccea0873-879d-4fcb-a1a0-d97197370751",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d835f1b-7493-4a1b-8c51-26142777d17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero shot with mlx-community/Qwen2.5-14B-Instruct-4bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6bd769e15f4937964f4aafaf0ca272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [04:10<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "for model in MODEL_MLX[2:3]:\n",
    "    print(f\"Zero shot with {model}\")\n",
    "    mlx_zero(model)\n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4e774-b71b-4c61-a04f-9cd3c0018eb5",
   "metadata": {},
   "source": [
    "## Clean Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3904bc0-b746-400e-b5f0-4988b10d459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736c97d6-1430-42bc-88f2-58f47e331c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relations(predicted, correct):\n",
    "    \"\"\"\n",
    "        compute precision, recall, f1\n",
    "\n",
    "        and score from 0 to 5 assigned this way:\n",
    "            - 1 point if relation exists\n",
    "            - up to 5 points for the cardinality\n",
    "    \"\"\"\n",
    "\n",
    "    total_score = 0\n",
    "    true_pos = 0\n",
    "    tp_fn = len(correct)\n",
    "    tp_fp = len(predicted)\n",
    "    \n",
    "    for rel in correct:\n",
    "        score = 0\n",
    "        for rel_ in predicted: \n",
    "            if set(rel.keys()) == set(rel_.keys()):\n",
    "                true_pos += 1\n",
    "                score += 1\n",
    "                for r in rel.keys():\n",
    "                    ## unify cardinality format\n",
    "                    if \"..\" not in rel[r]:\n",
    "                        rel[r] = '\"' + rel[r].replace('\"','') + \"..\"+ rel[r].replace('\"','') +'\"'\n",
    "\n",
    "                    if \"..\" not in rel_[r]:\n",
    "                        rel_[r] = '\"' + rel_[r].replace('\"','') + \"..\"+ rel_[r].replace('\"','') + '\"'\n",
    "        \n",
    "                    c_min = rel[r].split(\"..\")[0]\n",
    "                    c_min_ = rel_[r].split(\"..\")[0]\n",
    "\n",
    "                    if c_min == c_min_:\n",
    "                        score+=1\n",
    "\n",
    "                    c_max = rel[r].split(\"..\")[1]\n",
    "                    c_max_ = rel_[r].split(\"..\")[1]\n",
    "\n",
    "                    if c_max == c_max_:\n",
    "                        score+=1\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    if true_pos > 0 :\n",
    "        precision = true_pos / tp_fp \n",
    "        recall = true_pos / tp_fn \n",
    "        f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "    else:\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1 = 0\n",
    "    \n",
    "    return {\n",
    "        \"total_score\":total_score,\n",
    "        \"precision\": round(precision,2),\n",
    "        \"recall\": round(recall,2),\n",
    "        \"f1\": round(f1,2),\n",
    "        \"len\":len(correct) * 5\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f5aec6b-ca58-427a-be6f-2921d5f41417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class(predicted, correct):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, and F1-score between the predicted and correct lists.\n",
    "\n",
    "    Args:\n",
    "        predicted (list): The list of predicted values.\n",
    "        correct (list): The list of correct (ground truth) values.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "\n",
    "    # precision: how many predicted that are in correct.\n",
    "    if type(predicted) != type(set()):\n",
    "        precision =  (len(set(predicted).intersection(set(correct)))) / len(set(predicted))\n",
    "    else:\n",
    "        precision =  (len((predicted).intersection((correct)))) / len((predicted))\n",
    "\n",
    "    \n",
    "    # recall : how many predicted are not in correct.\n",
    "    if type(predicted) != type(set()):\n",
    "        recall =  (len(set(predicted).intersection(set(correct)))) / len(set(correct)) \n",
    "    else:\n",
    "        recall = (len((predicted).intersection((correct)))) / len((correct)) \n",
    "\n",
    "    # f1: classic f1\n",
    "    f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "    return {\n",
    "        \"precision\": round(precision,2),\n",
    "        \"recall\": round(recall,2),\n",
    "        \"f1\": round(f1,2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26dd9909-0ba3-44cd-93a8-ba1a2322291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def init_parser():\n",
    "    with open(\"/Users/marcocalamo/Downloads/grammar.ebnf\", encoding=\"utf-8\") as grammar_file:\n",
    "        parser = Lark(grammar_file.read())\n",
    "        return parser\n",
    "        \n",
    "def parse_text(parser, ref):\n",
    "    ref = ref.replace('`','')\n",
    "    ref = ref.replace('plantuml','')\n",
    "\n",
    "    if not '@startuml' in ref:\n",
    "        ref = '@startuml\\n' + ref\n",
    "    if not '@enduml' in ref:\n",
    "        ref = ref+'\\n@enduml'\n",
    "\n",
    "    ref = re.sub(r\"<think>.*?</think>\", \"\", ref, flags=re.DOTALL)\n",
    "    \n",
    "    #ref = ref.replace('@startuml','')\n",
    "    #ref = ref.replace('@enduml','')\n",
    "    parsed_ref = parser.parse(ref)\n",
    "    return parsed_ref\n",
    "\n",
    "def get_from_parsed(parsed_text, to_get=\"class\"):\n",
    "    found = parsed_text.find_pred(lambda v: v.data.value == to_get)\n",
    "    \n",
    "    to_ret = []\n",
    "\n",
    "    for f in found:\n",
    "        if to_get == 'relationship':\n",
    "            one_rel = f.children[0].children            \n",
    "            first_rel = one_rel[0].children[0].value\n",
    "            assert one_rel[0].children[0].type == 'CNAME'\n",
    "\n",
    "            try:\n",
    "                first_card = one_rel[1].children[0].value\n",
    "                assert one_rel[1].children[0].type == 'ESCAPED_STRING'\n",
    "            except:\n",
    "                first_card = \"0..*\"\n",
    "\n",
    "            try:\n",
    "                second_card = one_rel[5].children[0].value\n",
    "                assert one_rel[5].children[0].type == 'ESCAPED_STRING'\n",
    "            except:\n",
    "                second_card = \"0..*\"\n",
    "\n",
    "            second_rel = one_rel[-2].children[0].value\n",
    "            assert one_rel[-2].children[0].type == 'CNAME'\n",
    "            \n",
    "            rel_tuple = {\n",
    "                first_rel:first_card,\n",
    "                second_rel:second_card\n",
    "            }\n",
    "            \n",
    "            to_ret.append(rel_tuple)\n",
    "        elif to_get == 'class':\n",
    "            to_ret.append(f.children[0].children[0].value)\n",
    "    \n",
    "    return to_ret\n",
    "    \n",
    "def parse_path(path_to_uml, parser):\n",
    "    \n",
    "    if os.path.isfile(path_to_uml):\n",
    "        with open(path_to_uml) as plant_ref:\n",
    "            \n",
    "            ref = plant_ref.read()\n",
    "            parsed_ref = parse_text(parser, ref)\n",
    "            found_class = get_from_parsed(parsed_ref, to_get=\"class\")\n",
    "            found_rel = get_from_parsed(parsed_ref, to_get=\"relationship\")\n",
    "\n",
    "            return found_class, found_rel\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "014ca02c-77a5-4e91-bec1-3fc63aef5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52b950a5-85f0-46ba-9377-cb44b175bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_path(\"LouvreMuseum/uml.txt\", init_parser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3abbf13d-a8b7-4056-b7f3-f918ed522155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(path_uml, path_to_check, parser):\n",
    "\n",
    "    # gold class and rel\n",
    "    class_uml, rel_uml = parse_path(path_uml, parser)\n",
    "\n",
    "    # candidate class and rel\n",
    "    class_check, rel_check = parse_path(path_to_check, parser)\n",
    "\n",
    "    # check class\n",
    "    res_cl = check_class(class_check, class_uml)\n",
    "\n",
    "    # check rel\n",
    "    res_re = check_relations(rel_check, rel_uml)\n",
    "\n",
    "    return res_cl, res_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24eadd67-a4b2-4f3c-84f3-62f2129a9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(\"FactoriesAndProducts/uml.txt\", \"FactoriesAndProducts/result_gpt-4o-mini.txt\",init_parser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b66c99e3-45c4-4319-a353-48798542707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def process_folders(root_folder):\n",
    "    csv_file = os.path.join(root_folder, \"evaluation_results.csv\")\n",
    "    header_written = False\n",
    "    \n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        for sub_folder in os.listdir(root_folder):\n",
    "            sub_folder_path = os.path.join(root_folder, sub_folder)\n",
    "            if not os.path.isdir(sub_folder_path):\n",
    "                continue\n",
    "\n",
    "            uml_file = os.path.join(sub_folder_path, \"uml.txt\")\n",
    "            if not os.path.exists(uml_file):\n",
    "                continue\n",
    "            \n",
    "            for filename in os.listdir(sub_folder_path):\n",
    "                if filename.startswith(\"result_\") and filename.endswith(\".txt\"):\n",
    "                    result_file = os.path.join(sub_folder_path, filename)\n",
    "                    model_name = \"-\".join(filename.replace(\".txt\", \"\").split(\"_\")[1:])\n",
    "                    parser = init_parser()\n",
    "                    try:\n",
    "                        results = evaluate(uml_file, result_file, parser)\n",
    "                        if not header_written:\n",
    "                            writer.writerow([\"sub_folder_name\", \"model_name\", \n",
    "                                             \"precision_class\", \"recall_class\", \"f1_class\",\n",
    "                                             \"precision_rel\", \"recall_rel\", \"f1_rel\", \"score_rel\", \"max_score\"])\n",
    "                            header_written = True\n",
    "                        writer.writerow([sub_folder, model_name, results[0][\"precision\"], results[0][\"recall\"], results[0][\"f1\"],\n",
    "                                        results[1][\"precision\"], results[1][\"recall\"], results[1][\"f1\"],\n",
    "                                        results[1][\"total_score\"], results[1][\"len\"]])\n",
    "                    except Exception as e:\n",
    "                        print(e,sub_folder, model_name)\n",
    "                        #raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "396b5007-a0ab-4327-8064-226c1aac489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_folders(ROOT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9450d15-b93e-4b51-b264-6414382db18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_class(csv):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv)\n",
    "   # Compute average f1_class and f1_rel per model_name, and count instances\n",
    "    avg_metrics = df.groupby('model_name').agg(\n",
    "        f1_class_avg=('f1_class', 'mean'),\n",
    "        count=('model_name', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Set seaborn style\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    \n",
    "    # Plot average f1_class per model with count annotations\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.barplot(x='model_name', y='f1_class_avg', data=avg_metrics, palette='husl', hue='model_name', legend=False)\n",
    "    plt.title('Average F1 Class per Model')\n",
    "    plt.xlabel('Model Name')\n",
    "    plt.ylabel('Average F1 Class')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotate counts on bars\n",
    "    for index, row in avg_metrics.iterrows():\n",
    "        ax.text(index, row.f1_class_avg, f'N={row[\"count\"]}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_rel(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Compute average f1_class and f1_rel per model_name, and count instances\n",
    "    avg_metrics = df.groupby('model_name').agg(\n",
    "        f1_class_avg=('f1_rel', 'mean'),\n",
    "        count=('model_name', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Set seaborn style\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    \n",
    "    # Plot average f1_class per model with count annotations\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.barplot(x='model_name', y='f1_class_avg', data=avg_metrics, palette='husl', hue='model_name', legend=False)\n",
    "    plt.title('Average F1 Rel per Model')\n",
    "    plt.xlabel('Model Name')\n",
    "    plt.ylabel('Average F1 Rel')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotate counts on bars\n",
    "    for index, row in avg_metrics.iterrows():\n",
    "        ax.text(index, row.f1_class_avg, f'N={row[\"count\"]}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data_score(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Compute average f1_class and f1_rel per model_name, and count instances\n",
    "    avg_metrics = df.groupby('model_name').agg(\n",
    "        f1_class_avg=('score_rel', 'mean'),\n",
    "        count=('model_name', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Set seaborn style\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    \n",
    "    # Plot average f1_class per model with count annotations\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = sns.barplot(x='model_name', y='f1_class_avg', data=avg_metrics, palette='husl',hue='model_name', legend=False)\n",
    "    plt.title('Average Score rel per Model')\n",
    "    plt.xlabel('Model Name')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Annotate counts on bars\n",
    "    for index, row in avg_metrics.iterrows():\n",
    "        ax.text(index, row.f1_class_avg, f'N={row[\"count\"]}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a183a6e-a19a-438d-a271-b7a38b0bfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_class(\"evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fa21926-91d2-49ed-b56d-ec181eb56875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_class(\"/evaluation_results_chat_zero.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df36fc7e-6e81-468d-bb7b-5b9041725fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_rel(\"evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23a7acc-719e-4f62-a4b1-e9c9abd3fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_rel(\"evaluation_results_chat_zero.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d15087-c66d-47da-b39b-e4bc7517155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_score(\"evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98f802ed-5b7f-4bd8-97a0-61a3fc4cf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_data_score(\"evaluation_results_chat_zero.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
